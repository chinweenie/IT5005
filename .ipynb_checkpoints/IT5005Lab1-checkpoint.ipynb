{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IT5005 Artificial Intelligence\n",
    "# Lab 1\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to create dense neural networks using Keras.\n",
    "    2. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    3. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    \n",
    "To save time we will train each experiment only for 50 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "**HINT: YOU CAN HIT SHIFT-ENTER TO RUN EACH CELL. NOTE THAT IF A CELL IS DEPENDENT ON A PREVIOUS CELL, YOU WILL NEED TO RUN THE PREVIOUS CELL(S) FIRST **\n",
    "\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 1, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 1 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Keras provides the to_categorical function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(y = iris.target, num_classes = 3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                    random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Create the neural network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "nn.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd = SGD(lr = 0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn.compile(loss='categorical_crossentropy', optimizer=sgd, \n",
    "          metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 50 epochs. We will shuffle the training data between epochs, and provide validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 1.0919 - accuracy: 0.3250 - val_loss: 1.0840 - val_accuracy: 0.2000\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0207 - accuracy: 0.5500 - val_loss: 1.0447 - val_accuracy: 0.4000\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9742 - accuracy: 0.6250 - val_loss: 1.0112 - val_accuracy: 0.5667\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.9368 - accuracy: 0.6917 - val_loss: 0.9783 - val_accuracy: 0.5667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.9050 - accuracy: 0.6833 - val_loss: 0.9387 - val_accuracy: 0.5667\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.8738 - accuracy: 0.6917 - val_loss: 0.8977 - val_accuracy: 0.5667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.8399 - accuracy: 0.6917 - val_loss: 0.8745 - val_accuracy: 0.5667\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8125 - accuracy: 0.6917 - val_loss: 0.8560 - val_accuracy: 0.5667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7829 - accuracy: 0.6917 - val_loss: 0.8171 - val_accuracy: 0.5667\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7535 - accuracy: 0.7000 - val_loss: 0.7929 - val_accuracy: 0.5667\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.7261 - accuracy: 0.7083 - val_loss: 0.7712 - val_accuracy: 0.5667\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7018 - accuracy: 0.7000 - val_loss: 0.7340 - val_accuracy: 0.6000\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6775 - accuracy: 0.7167 - val_loss: 0.7033 - val_accuracy: 0.6000\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6595 - accuracy: 0.7250 - val_loss: 0.6774 - val_accuracy: 0.6000\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6351 - accuracy: 0.7667 - val_loss: 0.6704 - val_accuracy: 0.6000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6162 - accuracy: 0.7417 - val_loss: 0.6451 - val_accuracy: 0.6000\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5950 - accuracy: 0.7667 - val_loss: 0.6344 - val_accuracy: 0.6000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5764 - accuracy: 0.7500 - val_loss: 0.6138 - val_accuracy: 0.6000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5613 - accuracy: 0.7750 - val_loss: 0.6063 - val_accuracy: 0.6000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5451 - accuracy: 0.7583 - val_loss: 0.5858 - val_accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.5316 - accuracy: 0.7750 - val_loss: 0.5707 - val_accuracy: 0.6000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5213 - accuracy: 0.8000 - val_loss: 0.5550 - val_accuracy: 0.6333\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5087 - accuracy: 0.7750 - val_loss: 0.5337 - val_accuracy: 0.7333\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4990 - accuracy: 0.8083 - val_loss: 0.5096 - val_accuracy: 0.7667\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4865 - accuracy: 0.8250 - val_loss: 0.4970 - val_accuracy: 0.7667\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4741 - accuracy: 0.8750 - val_loss: 0.5048 - val_accuracy: 0.7333\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4644 - accuracy: 0.8417 - val_loss: 0.5025 - val_accuracy: 0.6667\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4565 - accuracy: 0.8333 - val_loss: 0.4893 - val_accuracy: 0.7333\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4463 - accuracy: 0.8417 - val_loss: 0.4744 - val_accuracy: 0.7333\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4400 - accuracy: 0.8667 - val_loss: 0.4700 - val_accuracy: 0.7333\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4318 - accuracy: 0.8667 - val_loss: 0.4641 - val_accuracy: 0.7333\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.4232 - accuracy: 0.8417 - val_loss: 0.4410 - val_accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4192 - accuracy: 0.8583 - val_loss: 0.4272 - val_accuracy: 0.8667\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4122 - accuracy: 0.9083 - val_loss: 0.4464 - val_accuracy: 0.7667\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.4060 - accuracy: 0.8833 - val_loss: 0.4468 - val_accuracy: 0.7333\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3997 - accuracy: 0.8667 - val_loss: 0.4182 - val_accuracy: 0.8000\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3923 - accuracy: 0.8750 - val_loss: 0.3983 - val_accuracy: 0.9333\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3843 - accuracy: 0.9167 - val_loss: 0.3973 - val_accuracy: 0.9000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.3786 - accuracy: 0.9083 - val_loss: 0.3986 - val_accuracy: 0.8667\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3740 - accuracy: 0.9083 - val_loss: 0.3887 - val_accuracy: 0.9000\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3674 - accuracy: 0.9167 - val_loss: 0.3997 - val_accuracy: 0.7667\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3623 - accuracy: 0.9000 - val_loss: 0.3840 - val_accuracy: 0.8667\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3592 - accuracy: 0.9167 - val_loss: 0.3772 - val_accuracy: 0.8667\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3538 - accuracy: 0.9083 - val_loss: 0.3740 - val_accuracy: 0.8667\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.3471 - accuracy: 0.9167 - val_loss: 0.3542 - val_accuracy: 0.9333\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3436 - accuracy: 0.9250 - val_loss: 0.3480 - val_accuracy: 0.9333\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3376 - accuracy: 0.9167 - val_loss: 0.3441 - val_accuracy: 0.9333\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3334 - accuracy: 0.9667 - val_loss: 0.3674 - val_accuracy: 0.8667\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3413 - accuracy: 0.9000 - val_loss: 0.3449 - val_accuracy: 0.9333\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3232 - accuracy: 0.9333 - val_loss: 0.3424 - val_accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1448d0ad0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, Y_train, shuffle = True, epochs = 50, \n",
    "      validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. \n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://keras.io/api/optimizers/sgd/). What does the lr parameter do? \n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The documentation states that the momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. \n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and validation accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc. \n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    |               |                 |                   |\n",
    "|0.1     |               |                 |                   |\n",
    "|1.0     |               |                 |                   |\n",
    "|10.0    |               |                 |                   |\n",
    "|100     |               |                 |                   |\n",
    "|1000    |               |                 |                   |\n",
    "|10000   |               |                 |                   |\n",
    "| 100000 |               |                 |                   |\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. \n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. Note: Do not use the Nesterov parameter - Leave it as False.\n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. \n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |               |                 |                   |\n",
    "|0.01      |               |                 |                   |\n",
    "|0.1       |               |                 |                   |\n",
    "|1.0       |               |                 |                   |\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning?\n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Using Raw Unscaled Data\n",
    "\n",
    "We begin by using unscaled X and Y data. The code below will create 120 training samples and 30 testing samples (20% of the total of 150 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unscaled = iris.data\n",
    "Y_raw = iris.target\n",
    "X_utrain, X_utest, Y_utrain, Y_utest = train_test_split(X_unscaled, Y,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "Create a new neural network called \"nn2\" below using a single hidden layer of 100 neurons. Train using the data in X_utrain, X_utest and validate with Y_utrain and Y_utest. Again use the SGD optimizer with a learning rate of 0.1 and no momentum, and train for 50 epochs. ***(3 marks)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnter your code for Question 5 below\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enter your code for Question 5 below. To TA: 3 marks for code.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Question 5 continues)**\n",
    "\n",
    "Observe the training and validation error. Does not scaling the input affect the training? Why do you think this is so? What is the advantage of scaling? \n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conclusion\n",
    "\n",
    "In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
